{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "## Introduction\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality-reduction method that is typically used to transform a high-dimensional data set into a smaller-dimensional subspace prior to applying a machine learning algorithm on the data.\n",
    "\n",
    "\n",
    "Let $\\mathbf{x^i} = \\left[ x_1^i, x_2^i, \\cdots, x_M^i \\right]$ be a M-dimensional vector describing the $i$-th sample. The entire dataset of $N$ samples can be expressed as a $N \\times M$ matrix $X$, whose rows are the samples:\n",
    "\n",
    "\\begin{align}\n",
    "X &= \\left(\n",
    "\\begin{array}{cccc}\n",
    "x_1^1 & x_2^1  & \\ldots & x_M^1  \\\\\n",
    "x_1^2 & x_2^2  & \\ldots & x_M^2  \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_1^N & x_2^N  & \\ldots & x_M^N\n",
    "\\end{array} \\right) \n",
    "\\end{align}\n",
    "\n",
    "PCA aims to transform the original data in order to maximise its variance. In practice, the new representation of the data $Y$ is linearly related to the original one:\n",
    "\n",
    "\\begin{equation} \n",
    "Y = X \\cdot V\n",
    "\\end{equation}\n",
    "\n",
    "The columns of $V$, $\\{ \\mathbf{v}_1, \\cdots , \\mathbf{v}_M \\}$ are the principal components (PCs) of $X$, and correspond to the eigenvectors of the covariance matrix of $X$, which is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{cov} (X)= \\frac{1}{N} \\sum_n^{N} (x_n - \\mu_x) \\, (x_n - \\mu_x)\n",
    "\\end{equation}\n",
    "\n",
    "The covariance matrix, $\\text{cov} (X)$, describes all relationships between pairs of measurements in our dataset $X$.\n",
    "\n",
    "Because the covariance matrix is symmetric, the eigenvectors form an orthogonal set. The PCs (eigenvectors) correspond to the direction (in the original n-dimensional space) with the greatest variance in the data. Each eigenvector has a corresponding eigenvalue, indicating how much variance there is in the data along the corresponding eigenvector (or PC).\n",
    "\n",
    "The dimensionality reduction can be performed by removing the PCs with the lowest eigenvalues, thus throwing away redundant features with low information content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: creating PCA function\n",
    "\n",
    "In order to get better insight into PCA, we will create a Python script for PCA from scratch and then apply it on a simple toy data.\n",
    "\n",
    "<b>NOTE</b>: it is important to run all the steps (code cells) below to ensure correct execution.\n",
    "\n",
    "First, the following codes is necessary to import the useful part of Python tools and libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt # This imports matplotlib library for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the PCA function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X):\n",
    "    \n",
    "    # computes the covariance of X\n",
    "    # numpy covariance function assumes different ordering, so we transpose X\n",
    "    XCov = np.cov(X.T)\n",
    "    \n",
    "    # solves the eigenproblem and stores eigvals in D and eigvecs in V\n",
    "    D, V = np.linalg.eig(XCov)\n",
    "    \n",
    "    # perform the linear transformation - matrix-matrix multiplication\n",
    "    Yn = np.dot(X,V)\n",
    "\n",
    "    # return the eigenvector matrix V, the transformed data Yn, and the eigenvalues D\n",
    "    return V,Yn,D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 1: PCA for dimensionality reduction on wine chemical data\n",
    "\n",
    "In this tutorial we will use real experimental data, representing the chemical composition of about 178 wine samples, from three different cultivars in the same region in Italy.\n",
    "For each wine, the analysis determined the quantities of 13 constituents, listed below:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Feature No.</th>\n",
    "    <th>Wine Chemical Composition</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Alcohol</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Malic acid</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>Ash</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>Alcalinity of ash</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>Magnesium</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>Total phenols</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>Flavanoids</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>Nonavanoid phenols</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>Proanthocyanins</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>Color intensity</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>Hue</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>OD280/OD315 of diluted wines</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>Proline</td>\n",
    "  </tr>\n",
    "  <caption>Wine chemical composition*\n",
    "  \n",
    "  *Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science</caption>\n",
    "</table><br>\n",
    "\n",
    "The data file <i>./data/wineInputs.txt</i> contains 178 lines, each with the 13 entries representing the wine chemical composition (descriptor). The file  <i>./data/wineOutputs.txt</i> contains the type of each wine: either 1, 2 or 3.\n",
    "\n",
    "In this tutorial, the objective is to reduce the dimensionality of the descriptors by identifying and eliminating the redundant ones with PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define normalization function here, that is zscore function\n",
    "# It is also known as standard score\n",
    "def zscore(X): # z-score uses to normalise the data.\n",
    "    \n",
    "    # get the shape of the data matrix\n",
    "    [nX,mX] = X.shape\n",
    "    \n",
    "    # compute the mean of every column X\n",
    "    XMean = np.mean(X, axis=0)\n",
    "    \n",
    "    # compute standard deviation of each column\n",
    "    XStd = np.std(X,axis=0,ddof=1)\n",
    "    \n",
    "    # subtract the mean from each column\n",
    "    zX = X - np.kron(np.ones((nX,1)),XMean) # Z = [X - mX]\n",
    "    \n",
    "    # divide by the stdv\n",
    "    Zscore = np.divide(zX,XStd)\n",
    "    \n",
    "    return Zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load input/output data from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataIn = np.genfromtxt('../data/wineInputs.txt', delimiter=',')\n",
    "dataOut = np.genfromtxt('../data/wineOutputs.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply zscore normalization to create 'X' matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.24560191 -0.49801083 -0.82566806 -2.48380535  0.01809498  0.56704962\n",
      "  0.73156905 -0.81840757 -0.54319081 -0.29250058  0.40490575  1.11030134\n",
      "  0.9625261 ]\n",
      "[ 0.2456   -0.49801  -0.82567  -2.4838    0.018094  0.56705   0.73157\n",
      " -0.81841  -0.54319  -0.2925    0.40491   1.1103    0.96253 ]\n"
     ]
    }
   ],
   "source": [
    "X = zscore(dataIn)\n",
    "\n",
    "print X[1,:]\n",
    "print dataIn[1,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then Apply the  PCA to create V, Ypca and D matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcao = PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we try to obtain the normalized cumulative sum (cumsum) of eigenvalues\n",
    "assuming D are the eigenvalues from PCA. Can you think how to this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot normalized cumulative sum to understand the contributions of the obtained PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Normalized cumulative sum')\n",
    "plt.xlabel('# principal components')\n",
    "plt.ylabel('cumulative sum')\n",
    "\n",
    "#plt.plot(xrange(1,len(D)+1),idc,'bo') # re-plot the data\n",
    "plt.plot(range(1,len(D)+1),idc,'bo') # re-plot the data\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "## Introduction\n",
    "K-means is an example of unsupervised learning algorithms for clustering problems.\n",
    "Suppose we have a data set $X=\\{ \\mathbf{x}_1 \\cdots \\mathbf{x}_N \\}$ consisting of $N$ observations of a random $D$-dimensional Euclidean variable $\\mathbf{x}$. Our goal is to partition the data set into some number $K$ of clusters, located around their centroids $\\mathbf{m}_k=\\{\\mathbf{\\mu}_1 \\cdots \\mathbf{\\mu}_K \\}$. We assume that the value of $K$ is given.\n",
    "\n",
    "K-means clustering can be solved by Expectation Maximisation (EM) algorithm which consists of two steps: E-step and M-step.\n",
    "On each E-step, we find the Euclidean distance between $N$ data points and $K$ cluster centers. The most probable cluster for each data sample $\\mathbf{x}_n$ is the one with nearest centroid:\n",
    "\n",
    "\\begin{align}\n",
    "z_n^*=\\text{arg} \\, \\min\\limits_{k} \\parallel \\mathbf{x}_n - \\mathbf{\\mu}_k \\parallel^2\n",
    "\\end{align}\n",
    "\n",
    "The M-step updates each cluster center by computing the mean of all points assigned to it:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{\\mu}_k=\\frac{1}{N_k} \\sum_{n:z_n = k} \\mathbf{x}_n\n",
    "\\end{align}\n",
    "\n",
    "The pseudo-code of K-means clustering can be summarised as following:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "    Randomly select cluster centers ($\\mathbf{m}_k$) as initial centroids; <br/> \n",
    "<b>while</b> <i>centroids change</i> <b>do</b>:     \n",
    "&emsp; <b>E-step</b>: <br/> \n",
    "&emsp; Calculate the distance between each data point and cluster center (centroid); <br /> \n",
    "&emsp; Assign each data point to its closest cluster center (centroid): $z_n^*=\\text{arg} \\, \\min\\limits_{k} \\parallel \\mathbf{x}_n - \\mathbf{\\mu}_k \\parallel^2$; <br /> \n",
    "&emsp; Form K clusters by assigning each point to its closest centroid; <br /> \n",
    "<br/> \n",
    "&emsp; <b>M-step</b>: <br />\n",
    "&emsp; Update each cluster center by computing the mean of all points assigned to it: $\\mathbf{\\mu}_k=\\frac{1}{N_k} \\sum_{n:z_n = k} \\mathbf{x}_n$; <br /> \n",
    "<b>end</b> <br /> \n",
    "<b>Result</b>: Cluster indices of each data point (assignments)    \n",
    "  </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Clustering on a simple toy data\n",
    "In this subsection, we implement K-means to cluster a simple toy data. \n",
    "First, we import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import all necessary libraries\n",
    "from pyKmeans import Kmeans # This imports the Kmeans function (which we created)\n",
    "import numpy as np # This imports numerical python (numpy) library\n",
    "import matplotlib.pyplot as plt # This imports matplotlib library for plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a simple data based on two independent multivariate Gaussian distributions and we plot this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create synthetic data using two Gaussian distributions\n",
    "# First, we determine the mean and standard deviation of two Gaussian distributions\n",
    "mu1=np.array([5,2]); sigma1=np.array([[0.4,-0.0255],[-0.0255,0.2]])\n",
    "mu2=np.array([9,7]); sigma2=np.array([[0.10,0],[0,0.4]])\n",
    "\n",
    "# Second, we determine the number of data points on each Gaussian distribution\n",
    "N1=300; N2=100\n",
    "\n",
    "# Third, we add these properties into multivariate normal dist. function in numpy\n",
    "X1=np.random.multivariate_normal(mu1, sigma1, N1)\n",
    "X2=np.random.multivariate_normal(mu2, sigma2, N2)\n",
    "X = np.concatenate((X1, X2), axis=0) # combine X1 and X2 as data X\n",
    "\n",
    "# Fourth, we plot the synthetic data based on two Gaussian distribution function \n",
    "plt.plot(X[:,0],X[:,1],'bo',linewidth=2.0,markersize=4.0)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result in the Figure shows very well separated data.\n",
    "Of course, it is very easy to visualize the separation/group by human eye, but clustering aims to do this process automatically. K-means clustering is one of simple algorithm to cluster/group data sets.\n",
    "\n",
    "In this tutorial, the number of clusters K is assumed to be known. There are some automatic\n",
    "methods in determining K parameter, but they are not discussed here.\n",
    "\n",
    "We then apply <i>K-means</i> function on the data (if you are interested the detailed of the algorithm, you can take a look at <i>Kmeans.py</i>). The function returns the cluster indices as well as the cluster's\n",
    "centroids. Finally, we plot the data for each cluster using different colour.\n",
    "\n",
    "The produced Figure demonstrates how K-means algorithm successfully forms two clusters in the data.\n",
    "The above problem of course is very easy to visualize because the generated data set is very well separated. The next exercise will demonstrate more challenging problem. The exercise is a good example to understand how K-means can be useful in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part, we will apply K-means clustering algorithm to the above generated data\n",
    "\n",
    "K=2 # determine the number of cluster\n",
    "\n",
    "assignment1, mu_k = Kmeans(X,K)\n",
    "\n",
    "k1=np.argwhere(assignment1==1) # index of cluster 1\n",
    "k2=np.argwhere(assignment1==2) # index of cluster 2\n",
    "\n",
    "plt.plot(X[k1,0],X[k1,1],'ro',X[k2,0],X[k2,1],'go',linewidth=2.0,markersize=4.0)\n",
    "plt.plot(mu_k[0,0],mu_k[0,1],'bx',mu_k[1,0],mu_k[1,1],'bx',linewidth=20.0,markersize=20.0)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Exercise:\n",
    "# Bring the closer the means of Gaussian distribution, what is the effect of K-means clustering\n",
    "# Play around with the generated dataset and see K-means effect\n",
    "# Here we use K=2, because we know we generated the data from two Gaussian distributions, what happened if you change K\n",
    "\n",
    "# Homework:\n",
    "# Understand the K-means clustering function\n",
    "# Change the distance metrics, from Euclidean distance to be Manhattan distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: K-means clustering on chemical wine data\n",
    "\n",
    "This exercise is a continuation of previous wine exercise (PCA). \n",
    "We will use the wine data set, described in the previous notebook.\n",
    "\n",
    "Based on the eigenvalues generated by PCA in the data, it is known that there are three most dominant PCs. \n",
    "The objective is to apply K-means on the reduced data (using the first three PCs) for clustering three different groups of wine. \n",
    "We know the actual classiffication of the wine from the data set into three types, so we hope that the clusters generated through K-means mimic the actual wine types. \n",
    "\n",
    "### Exercise 1.a:\n",
    "We provide the template for this exercise below.\n",
    "Perform K-means on the reduced data (using only three PCs). Produce a 3D plot of the grouped wine data based on clustering and compare the results with the actual wine classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that we know the number of cluster, set variable 'K' to 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply K-means clustering on the first three PCs data (hint: PCs are stored in the Ypca matrix as columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the output and real classes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = pylab.figure()\n",
    "ax = Axes3D(fig1)\n",
    "ax.scatter(Ypca[dataOut==1,0],Ypca[dataOut==1,1],Ypca[dataOut==1,2],c='b')\n",
    "ax.scatter(Ypca[dataOut==2,0],Ypca[dataOut==2,1],Ypca[dataOut==2,2],c='r')\n",
    "ax.scatter(Ypca[dataOut==3,0],Ypca[dataOut==3,1],Ypca[dataOut==3,2],c='g')\n",
    "plt.title('Real Classes')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "# Plot the wine classes based on K-means clustering here\n",
    "fig2 = pylab.figure()\n",
    "ax = Axes3D(fig2)\n",
    "ax.scatter(Ypca[assignment1==1,0],Ypca[assignment1==1,1],Ypca[assignment1==1,2],c='b')\n",
    "ax.scatter(Ypca[assignment1==2,0],Ypca[assignment1==2,1],Ypca[assignment1==2,2],c='r')\n",
    "ax.scatter(Ypca[assignment1==3,0],Ypca[assignment1==3,1],Ypca[assignment1==3,2],c='g')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "plt.title('Kmeans')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "Make a simple function to count the number of correct group index (by comparing it with the true class data)\n",
    "\n",
    "### Exercise 2:\n",
    "Perform K-means clustering without applying PCA on the data and compare the results with the demo 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 3: K-means clustering on atmospheric variables\n",
    "\n",
    "Atmospheric new-particle formation (NPF) is an important source of climatically\n",
    "relevant atmospheric aerosol particles. \n",
    "NPF is the episodes where the ultra-fine aerosol particles appearing in the atmosphere and growing subsequently to larger sizes until they reach sizes where they potentially scatter solar irradiation and impact cloud condensation nuclei (CCN).\n",
    "This fact has motivated scientists across the world to study the atmospheric variables which contribute to the process of NPF. Below Figure shows two examples of new-particle formation events, the Figures on the left and right hand sides constitute non-event and event days, respectively.\n",
    "\n",
    "<table>\n",
    "  <caption align=\"bottom\">Examples of non-event and event days at Hyyti{\\\"a}l{\\\"a}, SMEAR II, Finland, in May 2005.</caption>\n",
    "  <tr><td>\n",
    "    <th><img src=\"../images/NonEventDay.png\" width=\"200px\"></th>\n",
    "    <th><img src=\"../images/EventDay.png\" width=\"200px\"></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "In this exercise, we have a data set consists of two daily averaged atmospheric variables: global radiation ($W/ms^{-2}$) and Relative Humidty (%). They were measured from January 1996 - December 2014 at Hyyti{\\\"a}l{\\\"a}, SMEAR II, Finland. \n",
    "The below figure shows the scatter plot between RH and Global\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using numpy to load file of relative humidity and solar radiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = np.loadtxt('../data/DataForClusteringFinal.txt', skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Relative Humidty (RH) and Global radiation to NPF\n",
    "\n",
    "We will use a matrix 'X' to represent the data which we will use as a unsupervised learning. X is column 2 and 3 of the *Data* above which is RH and global radiation.\n",
    "\n",
    "To check how our method performs, *Data* also contains our expected output which is known as *labels* in data mining speak. This is columns 4 (event day) and 5 (non-event day). Assign this expected output as 'Y'. \n",
    "\n",
    "\n",
    "Run the clustering on X. How does it compare to the real output Y? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
