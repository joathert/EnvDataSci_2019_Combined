{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Introduction \n",
    "\n",
    "In this session we will outline a basic environmental data work-flow. Our goal is to highlight common data tasks, and typical ways to solve them in Python.\n",
    "\n",
    "When working with environmental data, there are usually a few steps that come up each time. These are:\n",
    "\n",
    "- **reading**. Typically data is read from text files, but can also come from the internet or in other format p\n",
    "- **processing**. The data we read is usually a little untidy , for example we may need to subset to correct dates.\n",
    "- **plotting**. Plotting data is always worth doing as early as possible. Use histograms or simple line plots as your first steps in visualising data.\n",
    "\n",
    "In this notebook we will work through each step in turn with example data. We will once again work with data downloaded from SMARTSMEAR, in this case we will use flux data measured using the eddy co-variance technique at SMEARII research station. We will use Temperature which is measured at 16.8m Height, Gross Primary Production (GPP) which is derived from measurements of CO2 exchange, and Evapotranspiration (ET) which is derived from measurements of H2O exchange.\n",
    "\n",
    "To do these efficiently in Python we need to use additional software libraries (packages of pre-written code). The main package that we will use is **Pandas** which is for data analysis. Note that a great deal of Pandas functionality is very similar to R, in fact Pandas inspired by R's success. \n",
    "\n",
    "Before we start there is one other thing we should mention.In this session we will assume that terms like function, argument are familiar to you. If they are not then go back to R1-introduction.ipynb, and check the definition. If you cannot find the definition in there then complain to your instructors to update the intro! Alright, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading\n",
    "\n",
    "Our first task is to read in our GPP, ET and temperature data. Reading data takes data from storage (typically your computer's hard disk) and places it somewhere (in RAM) that is can be operated on by R. We have already downloaded our data as two seperate text files from SMARTSMEAR, and stored these files in the /data directory (folder) on github: https://github.com/OptPhotLab/EnvDataSciNotebooks/tree/master/data (You can inspect the data files by clicking the github link, but opening the individual files on github could slow your computer down!).\n",
    "\n",
    "We will use the Pandas **read.csv** function to read our data. Whenever we use an external library, we first need to import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line above imports pandas to our working environment. The convention is to use the *as* statement to (locally) rename the module to *pd* as it is shorted to write. Everytime we need to use pandas code we first type *pd* then we access functionality using an additional dot *.* and the function name. For example, to read in the GPP data use the *read_csv* function:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpp = pd.read_csv(\"../data/gppsmeardata_20160101120000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and read the ET data, which you can find in the same location, using the *read_csv* method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in R, we can use *head* function to inspect our objects (*gpp*). However, unlike *R* we use the dot operator to access the method directly on the object itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is going on here? *gpp* is a pandas dataframe object that contains not just data that we read in, but also *methods* (functions) that operate on this data. This concept is one of the core ideas behind [object orientated programming](https://www.datacamp.com/community/tutorials/python-oop-tutorial). The main thing to [grok](https://en.wikipedia.org/wiki/Grok) at this point is that in python *everything* is an object!  \n",
    "\n",
    "We can inspect the attributes, methods and data of any object using the [*Built-in*](https://docs.python.org/3/library/functions.html) function *dir()*. Try it below:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you scroll past the underscrored names at the start, then you will see that attributes are listed in alphabetical orders, you can see *head* part way down the list. So what's the idea with the underscores? You can more about that [here](https://stackoverflow.com/questions/1301346/what-is-the-meaning-of-a-single-and-a-double-underscore-before-an-object-name). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing\n",
    "\n",
    "Before we can make any graphs or perform any stats we usually have to tidy our data\n",
    "and there are a bunch of techniques in R that can help out with this. Let's \n",
    "check out a few of them that make life easier.\n",
    "\n",
    "## 2.1. Merging and renaming columns \n",
    "\n",
    "We have read in *two* different data files which have same first six columns. We can make life easier by combining these into a single dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpp_et = gpp.merge(et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *head* to check the combination worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read the third file (*T168_20160101120000.csv*), which is temperature data, into Python. When you have read it in with name *t*, merge it with *gpp_et* to make *gpp_et_t* dataframe:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIP for t then read.csv\n",
    "# TIP apply merge on gpp_et\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of the columns are a little long, we can shorten them to make our life easier:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpp_et_t.rename(columns={'HYY_EDDY233.GPP':'GPP','HYY_EDDY233.ET_gapf':'ET',\n",
    "                         'HYY_META.T168':'T168'}, inplace=True)\n",
    "gpp_et_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Subsetting\n",
    "Often we download much more data than we need. We use *indexing* to subset data. We can index data by selecting data from a single column which meets a specific condition. We select a single column using its name (as a string) and [] indexing brackets, try it out for the *month* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we were interested in data from September, we can use this month column with a logical comparison (*==*) to subset the dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sep = gpp_et_t[(gpp_et_t['Month']==9)]\n",
    "data_sep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you create a new dataframe containing data measured close to midday (when hour is between 10 and 15 o'clock)? You will need to chain multiple logical comparisons using the *&* operator.\n",
    "\n",
    "Name this dataframe *data_midday*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dates and indexing\n",
    "\n",
    "The data in the first 6 columns is all related to time. We can combine this data into a single datetime variable which will be useful for plotting later on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_datetimes = pd.to_datetime(data_midday.Year*1e8 +\n",
    "                                data_midday.Month*1e6 +\n",
    "                                data_midday.Day*1e4 +\n",
    "                                data_midday.Hour*1e2 +\n",
    "                                data_midday.Minute,\n",
    "                                format='%Y%m%d%H%M')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set this as the dataframe index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_midday.index = data_datetimes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Plotting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has plotting built in. Hence the objects we have created have plotting methods available. The most basic graph is a line graph using *.plot()*. To apply this method to only a single column (e.g. *GPP*) first index that column to select it and next apply the method. You can do this in either 1 or 2 lines of code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also make scatter plots in pandas from two columns and the plot.scatter method, try it out below on GPP and Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas** actually uses the **matplotlib** library to do its plotting. You can also apply this lib in a more conventional way, by using functions imported directly from the library. This is useful, if for example, you want to visualise some data but do not want to use Pandas. \n",
    "\n",
    "To get started, first we import the library using the following slightly unusual syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the *plt.plot()* function to plot our line plot above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_midday['GPP'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
